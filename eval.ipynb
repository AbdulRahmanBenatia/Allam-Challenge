{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "def classification_task(df):\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy loss and accuracy for a classification task.\n",
    "    \n",
    "    Parameters:d\n",
    "    df (pandas.DataFrame): DataFrame with columns for ground truth class and predicted class.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Cross-entropy loss, accuracy\n",
    "    \"\"\"\n",
    "    y_true = df.iloc[:, 1].values\n",
    "    y_pred = df.iloc[:, 2].values\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    # ce_loss = log_loss(y_true, y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true, np.round(y_pred))\n",
    "    \n",
    "    # return ce_loss, acc\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from datasets import load_metric\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "def generation_task(df):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for a text generation task.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with columns for original and generated text.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing BERT score, BLEU, GLUE, and perplexity.\n",
    "    \"\"\"\n",
    "    original_texts = df.iloc[:, 0].tolist()\n",
    "    generated_texts = df.iloc[:, 1].tolist()\n",
    "    \n",
    "    # Calculate BERT score\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_scorer = pipeline('text-similarity', model=model, tokenizer=tokenizer)\n",
    "    bert_score = np.mean([bert_scorer(orig, gen)['similarity'] for orig, gen in zip(original_texts, generated_texts)])\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    bleu = corpus_bleu([[ref] for ref in original_texts], [hyp for hyp in generated_texts])\n",
    "    \n",
    "    # Calculate GLUE\n",
    "    glue_metric = load_metric('glue', 'stsb')\n",
    "    glue_score = glue_metric.compute(predictions=generated_texts, references=original_texts)['pearson']\n",
    "    \n",
    "    # Calculate perplexity using LLaMA\n",
    "    # llama_tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "    # llama_model = LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "\n",
    "    # def calculate_perplexity(text):\n",
    "    #     input_ids = llama_tokenizer.encode(text, return_tensors='pt')\n",
    "    #     with torch.no_grad():\n",
    "    #         output = llama_model(input_ids, labels=input_ids)[0]\n",
    "    #     return torch.exp(output).item()\n",
    "    \n",
    "    # perplexity = np.mean([calculate_perplexity(gen) for gen in generated_texts])\n",
    "    \n",
    "    return {\n",
    "        'bert_score': bert_score,\n",
    "        'bleu': bleu,\n",
    "        'glue': glue_score,\n",
    "        # 'perplexity': perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ibm_API import get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_data_df = pd.read_json(\"test_gen.json\", encoding=\"utf-8\")\n",
    "generation_data_df[\"MGT\"] = \"MGT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>MGT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>دع الأمانيّ أو رُمهنّ من ظُبةٍ فإنما هنّ من غي...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>لا بل هوَ النورُ أضحى يَـــدِبُّ فـــي الظــلم...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>لما سلكوا بالهجرِ يوماً طريقتي سـيَـنْـدم بُـع...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>ما أَنتَ بِالحَكَمِ الَّذي سُمِّيتَهُ غالَتكَ ...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>إِذا المَدحُ زانَ فَتى مَعشَرٍ فَإِنَّ يَزيدَ ...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>أَجْنَى إِلَيْهَا الرِّضَى جَنَانِي مِنَ الْمَ...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>وَلَوى بِــقَــلبــي مُــذ لَوى أَصــداغــه وَ...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>وأَرسلَتِ اللَّحظَ الضَّعيفَ مع الهَوى لِيَقوى...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>ألا فـي سـبـيـل الله فـقـد أخى تقى دعـاه إلى ا...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>\\n        أنت شاعر فصيح عليم بقواعد العروض وال...</td>\n",
       "      <td>يطوي الليالي ساهراً متعذبا مـتـضـرعـاً مـن صـد...</td>\n",
       "      <td>MGT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input  \\\n",
       "0    \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "1    \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "2    \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "3    \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "4    \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "..                                                 ...   \n",
       "195  \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "196  \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "197  \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "198  \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "199  \\n        أنت شاعر فصيح عليم بقواعد العروض وال...   \n",
       "\n",
       "                                                output  MGT  \n",
       "0    دع الأمانيّ أو رُمهنّ من ظُبةٍ فإنما هنّ من غي...  MGT  \n",
       "1    لا بل هوَ النورُ أضحى يَـــدِبُّ فـــي الظــلم...  MGT  \n",
       "2    لما سلكوا بالهجرِ يوماً طريقتي سـيَـنْـدم بُـع...  MGT  \n",
       "3    ما أَنتَ بِالحَكَمِ الَّذي سُمِّيتَهُ غالَتكَ ...  MGT  \n",
       "4    إِذا المَدحُ زانَ فَتى مَعشَرٍ فَإِنَّ يَزيدَ ...  MGT  \n",
       "..                                                 ...  ...  \n",
       "195  أَجْنَى إِلَيْهَا الرِّضَى جَنَانِي مِنَ الْمَ...  MGT  \n",
       "196  وَلَوى بِــقَــلبــي مُــذ لَوى أَصــداغــه وَ...  MGT  \n",
       "197  وأَرسلَتِ اللَّحظَ الضَّعيفَ مع الهَوى لِيَقوى...  MGT  \n",
       "198  ألا فـي سـبـيـل الله فـقـد أخى تقى دعـاه إلى ا...  MGT  \n",
       "199  يطوي الليالي ساهراً متعذبا مـتـضـرعـاً مـن صـد...  MGT  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    generation_data_df.iloc[i, 2] = get_response(generation_data_df.iloc[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        أنت شاعر فصيح عليم بقواعد العروض والقافية، تكتب الشعر ملتزما بها.\\n        اكتب أبياتًا بعد هذا البيت، ملتزما بالبحر والقافية باحتراف:\\n        \\n                                        اكتب أبياتًا بعد هذا البيت: \\n                                        إِن صَوَّروكَ فَإِنَّما قَد صَوَّروا تاجَ الفَخارِ وَمَطلَعَ الأَنوارِ\\n                                        \\n        '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_data_df.iloc[10,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = ['الخفيف',\n",
    " 'مجزوء الرمل',\n",
    " 'البسيط',\n",
    " 'الكامل',\n",
    " 'الوافر',\n",
    " 'الطويل',\n",
    " 'السريع',\n",
    " 'المنسرح',\n",
    " 'مجزوء الكامل',\n",
    " 'المجتث',\n",
    " 'الرمل',\n",
    " 'مجزوء الوافر',\n",
    " 'المتقارب',\n",
    " 'مخلع البسيط',\n",
    " 'مجزوء الرجز',\n",
    " 'مجزوء الخفيف',\n",
    " 'الرجز',\n",
    " 'المديد',\n",
    " 'الهزج',\n",
    " 'مجزوء البسيط',\n",
    " 'منهوك المنسرح',\n",
    " 'أحذ الكامل',\n",
    " 'مشطور الرجز',\n",
    " 'المضارع',\n",
    " 'المقتضب',\n",
    " 'مجزوء المتقارب',\n",
    " 'مجزوء السريع',\n",
    " 'منهوك الرجز']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df = pd.read_json(\"test_cls_as_trained.json\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inputs to try later on other models:\n",
    "\n",
    "# classification_data_df['input'].to_list()\n",
    "with open('test_cls_for_other_LLMs.json', 'w', encoding='utf-8') as txt_file:\n",
    "    for item in classification_data_df['input'].to_list():\n",
    "        txt_file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'] = 'Pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>Base Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ما هو البحر الشعري لهذا البيت؟ وطـال ليـلى ودم...</td>\n",
       "      <td>البسيط</td>\n",
       "      <td>Pred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ما هو البحر الشعري لهذا البيت؟ فَـلا تَـقـطَـع...</td>\n",
       "      <td>الطويل</td>\n",
       "      <td>Pred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ما هو البحر الشعري لهذا البيت؟ كـم عـزمـةٍ يُـ...</td>\n",
       "      <td>الكامل</td>\n",
       "      <td>Pred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ما هو البحر الشعري لهذا البيت؟ سَلِ الفَريقَ ا...</td>\n",
       "      <td>البسيط</td>\n",
       "      <td>Pred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ما هو البحر الشعري لهذا البيت؟ فـــولّى يُـــب...</td>\n",
       "      <td>الطويل</td>\n",
       "      <td>Pred</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  output Base Prediction\n",
       "0  ما هو البحر الشعري لهذا البيت؟ وطـال ليـلى ودم...  البسيط            Pred\n",
       "1  ما هو البحر الشعري لهذا البيت؟ فَـلا تَـقـطَـع...  الطويل            Pred\n",
       "2  ما هو البحر الشعري لهذا البيت؟ كـم عـزمـةٍ يُـ...  الكامل            Pred\n",
       "3  ما هو البحر الشعري لهذا البيت؟ سَلِ الفَريقَ ا...  البسيط            Pred\n",
       "4  ما هو البحر الشعري لهذا البيت؟ فـــولّى يُـــب...  الطويل            Pred"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classification_data_df)):\n",
    "    classification_data_df.iloc[i,2] = get_response(classification_data_df.iloc[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying 200 samples took 1m 11.0s --> 0.355s for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type str which has no callable rint method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'rint'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type str which has no callable rint method",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'rint'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclassification_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassification_data_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m, in \u001b[0;36mclassification_task\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate cross-entropy loss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# ce_loss = log_loss(y_true, y_pred)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# return ce_loss, acc\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
      "File \u001b[1;32mc:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3360\u001b[0m, in \u001b[0;36mround\u001b[1;34m(a, decimals, out)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_round_dispatcher)\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mround\u001b[39m(a, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3272\u001b[0m \u001b[38;5;124;03m    Evenly round to the given number of decimals.\u001b[39;00m\n\u001b[0;32m   3273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3358\u001b[0m \n\u001b[0;32m   3359\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mround\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:68\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type str which has no callable rint method"
     ]
    }
   ],
   "source": [
    "classification_task(classification_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base = classification_data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort them to avoid taking the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_sorted = sorted(meters, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in classification_data_df['Base Prediction'].values:\n",
    "#     for meter in meters_sorted: \n",
    "#         if meter in res:\n",
    "            \n",
    "\n",
    "# cls_df_base.loc[cls_df_base['Base Prediction'] in , 'Base Prediction'] = cls_df_base['Base Res'] * 2\n",
    "changed = []\n",
    "for idx, res in enumerate(classification_data_df['Base Prediction'].values):\n",
    "    for meter in meters_sorted:\n",
    "        if meter in res:\n",
    "            ch_dict = {'Index': idx, \"Before\": res, 'After': meter}\n",
    "            changed.append(ch_dict)\n",
    "            classification_data_df.at[idx, 'Base Prediction'] = meter\n",
    "print(changed)\n",
    "print(len(changed))\n",
    "print(len(classification_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Base Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy without Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Base Prediction'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_pattern = '[\\u0621-\\u064A]'\n",
    "\n",
    "# Filter rows where 'Base Prediction' does not contain Arabic letters\n",
    "cls_df_without_arabic = classification_data_df[classification_data_df['Base Prediction'].apply(lambda x: not bool(re.search(arabic_pattern, x)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = cls_df_without_empty['output'] == cls_df_without_empty['Base Prediction']\n",
    "accuracy_without_empty = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model (Input refined for API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df = pd.read_json(\"test_cls_final.json\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'] = 'Pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classification_data_df)):\n",
    "    classification_data_df.iloc[i,2] = get_response(classification_data_df.iloc[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying 200 samples took 1m 37.3s --> 0.355s for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base = classification_data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort them to avoid taking the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_sorted = sorted(meters, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in classification_data_df['Base Prediction'].values:\n",
    "#     for meter in meters_sorted: \n",
    "#         if meter in res:\n",
    "            \n",
    "\n",
    "# cls_df_base.loc[cls_df_base['Base Prediction'] in , 'Base Prediction'] = cls_df_base['Base Res'] * 2\n",
    "changed = []\n",
    "for idx, res in enumerate(classification_data_df['Base Prediction'].values):\n",
    "    for meter in meters_sorted:\n",
    "        if meter in res:\n",
    "            ch_dict = {'Index': idx, \"Before\": res, 'After': meter}\n",
    "            changed.append(ch_dict)\n",
    "            classification_data_df.at[idx, 'Base Prediction'] = meter\n",
    "print(changed)\n",
    "print(len(changed))\n",
    "print(len(classification_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Base Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy without Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Base Prediction'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_pattern = '[\\u0621-\\u064A]'\n",
    "\n",
    "# Filter rows where 'Base Prediction' does not contain Arabic letters\n",
    "cls_df_without_arabic = classification_data_df[classification_data_df['Base Prediction'].apply(lambda x: not bool(re.search(arabic_pattern, x)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = cls_df_without_empty['output'] == cls_df_without_empty['Base Prediction']\n",
    "accuracy_without_empty = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_cls_formatted.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()  \n",
    "    \n",
    "fine_tuned_preds =  [line.strip() for line in lines]\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [\n",
    "    ' '.join(line.replace('\\n', '').split()) for line in fine_tuned_preds\n",
    "]\n",
    "\n",
    "print(fine_tuned_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [re.sub(r'[^\\u0600-\\u06FF]+', '', pred).strip() for pred in fine_tuned_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in fine_tuned_preds:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[\"Fine-Tuning Prediction\"] = fine_tuned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[classification_data_df['Fine-Tuning Prediction'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Fine-Tuning Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "wrong_rows_df = classification_data_df[~matches]\n",
    "print(\"Rows where col1 and col2 don't match:\")\n",
    "print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [\n",
    "    ' '.join(line.replace('\\n', '').split()) for line in fine_tuned_preds\n",
    "]\n",
    "\n",
    "print(fine_tuned_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [re.sub(r'[^\\u0600-\\u06FF]+', '', pred).strip() for pred in fine_tuned_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in fine_tuned_preds:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[\"Fine-Tuning Prediction\"] = fine_tuned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[classification_data_df['Fine-Tuning Prediction'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Fine-Tuning Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "wrong_rows_df = classification_data_df[~matches]\n",
    "print(\"Rows where col1 and col2 don't match:\")\n",
    "print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4o Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GPT_4o_Cls.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()  \n",
    "    \n",
    "gpt_preds =  [line.strip() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_3628\\2414898903.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  classification_data_df_100[\"GPT_4o Prediction\"] = gpt_preds\n"
     ]
    }
   ],
   "source": [
    "classification_data_df_100 = classification_data_df.head(100)\n",
    "\n",
    "classification_data_df_100[\"GPT_4o Prediction\"] = gpt_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.00%\n"
     ]
    }
   ],
   "source": [
    "matches = classification_data_df_100['output'] == classification_data_df_100['GPT_4o Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [\n",
    "    ' '.join(line.replace('\\n', '').split()) for line in fine_tuned_preds\n",
    "]\n",
    "\n",
    "print(fine_tuned_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [re.sub(r'[^\\u0600-\\u06FF]+', '', pred).strip() for pred in fine_tuned_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in fine_tuned_preds:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[\"Fine-Tuning Prediction\"] = fine_tuned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[classification_data_df['Fine-Tuning Prediction'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Fine-Tuning Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "wrong_rows_df = classification_data_df[~matches]\n",
    "print(\"Rows where col1 and col2 don't match:\")\n",
    "print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
