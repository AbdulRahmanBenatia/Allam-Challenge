{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 48,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "def classification_task(df):\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy loss and accuracy for a classification task.\n",
    "    \n",
    "    Parameters:d\n",
    "    df (pandas.DataFrame): DataFrame with columns for ground truth class and predicted class.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Cross-entropy loss, accuracy\n",
    "    \"\"\"\n",
    "    y_true = df.iloc[:, 0].values\n",
    "    y_pred = df.iloc[:, 1].values\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    ce_loss = log_loss(y_true, y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(y_true, np.round(y_pred))\n",
    "    \n",
    "    return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 49,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from datasets import load_metric\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "def generation_task(df):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for a text generation task.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with columns for original and generated text.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing BERT score, BLEU, GLUE, and perplexity.\n",
    "    \"\"\"\n",
    "    original_texts = df.iloc[:, 0].tolist()\n",
    "    generated_texts = df.iloc[:, 1].tolist()\n",
    "    \n",
    "    # Calculate BERT score\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_scorer = pipeline('text-similarity', model=model, tokenizer=tokenizer)\n",
    "    bert_score = np.mean([bert_scorer(orig, gen)['similarity'] for orig, gen in zip(original_texts, generated_texts)])\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    bleu = corpus_bleu([[ref] for ref in original_texts], [hyp for hyp in generated_texts])\n",
    "    \n",
    "    # Calculate GLUE\n",
    "    glue_metric = load_metric('glue', 'stsb')\n",
    "    glue_score = glue_metric.compute(predictions=generated_texts, references=original_texts)['pearson']\n",
    "    \n",
    "    # Calculate perplexity using LLaMA\n",
    "    # llama_tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "    # llama_model = LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "\n",
    "    # def calculate_perplexity(text):\n",
    "    #     input_ids = llama_tokenizer.encode(text, return_tensors='pt')\n",
    "    #     with torch.no_grad():\n",
    "    #         output = llama_model(input_ids, labels=input_ids)[0]\n",
    "    #     return torch.exp(output).item()\n",
    "    \n",
    "    # perplexity = np.mean([calculate_perplexity(gen) for gen in generated_texts])\n",
    "    \n",
    "    return {\n",
    "        'bert_score': bert_score,\n",
    "        'bleu': bleu,\n",
    "        'glue': glue_score,\n",
    "        # 'perplexity': perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 50,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ibm_API import get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_data_df = pd.read_json(\"test_gen.json\", encoding=\"utf-8\")\n",
    "generation_data_df[\"MGT\"] = \"MGT\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 52,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_data_df['output'] = generation_data_df['output'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE IN API FUNCTION: \n",
      " \n",
      "Reshaped RESPONSE IN API FUNCTION: \n",
      " \n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "for i in range(10):\n",
    "    generation_data_df.iloc[i, 2] = get_response(generation_data_df.iloc[i, 0])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اكتب أبياتًا بعد هذا البيت:  0         اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n1         اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n2         اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n3         اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n4         اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n                                ...                        \\n254625    اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n254626    اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n254627    اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n254628    اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\n254629    اكتب أبياتًا بعد هذا البيت:  <pandas.core.stri...\\nName: input, Length: 143945, dtype: object'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "generation_data_df.iloc[10,0]"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "cell_type": "markdown",
   "metadata": {},
>>>>>>> Stashed changes
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE IN API FUNCTION: \n",
      " \n",
      "Reshaped RESPONSE IN API FUNCTION: \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(generation_data_df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اكتب أبياتًا بعد هذا البيت:  0         أَصبَحَ المُلك لِلَّذي فَطر الخَل قَ بِتَقديرٍ...\\n1                    مِن أَي مَولى اِرتَجي وَلاي باب التَجي\\n2         العَبد عَبدك يا مَن أَنتَ سَيدهُ وَلَيسَ غَيرك...\\n3         لَو كُنتَ أَطمَع بِالمَنام تَوهما لَسالَت طَيف...\\n4         يعد عَليَّ أَنفاسي ذُنوباً إِذا ما قُلت أَفديه...\\n                                ...                        \\n254625                     وعندنا ليس محض إلى الشرق أن تولي\\n254626                       من الليالي نهار ومن عتمها ضياء\\n254627                      مقامه إذ تجلى وفي البيد إذ تحلى\\n254628                  على بابك انتظرنا وفي البال ألف حيلة\\n254629                      خذ الصفر لا تبالي فللصفر للرجال\\nName: input, Length: 143945, dtype: object'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_data_df.iloc[0,0]"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = ['الخفيف',\n",
    " 'مجزوء الرمل',\n",
    " 'البسيط',\n",
    " 'الكامل',\n",
    " 'الوافر',\n",
    " 'الطويل',\n",
    " 'السريع',\n",
    " 'المنسرح',\n",
    " 'مجزوء الكامل',\n",
    " 'المجتث',\n",
    " 'الرمل',\n",
    " 'مجزوء الوافر',\n",
    " 'المتقارب',\n",
    " 'مخلع البسيط',\n",
    " 'مجزوء الرجز',\n",
    " 'مجزوء الخفيف',\n",
    " 'الرجز',\n",
    " 'المديد',\n",
    " 'الهزج',\n",
    " 'مجزوء البسيط',\n",
    " 'منهوك المنسرح',\n",
    " 'أحذ الكامل',\n",
    " 'مشطور الرجز',\n",
    " 'المضارع',\n",
    " 'المقتضب',\n",
    " 'مجزوء المتقارب',\n",
    " 'مجزوء السريع',\n",
    " 'منهوك الرجز']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df = pd.read_json(\"test_cls_as_trained.json\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inputs to try later on other models:\n",
    "\n",
    "# classification_data_df['input'].to_list()\n",
    "with open('test_cls_as_trained.json', 'w', encoding='utf-8') as txt_file:\n",
    "    for item in classification_data_df['input'].to_list():\n",
    "        txt_file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'] = 'Pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classification_data_df)):\n",
    "    classification_data_df.iloc[i,2] = get_response(classification_data_df.iloc[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying 200 samples took 1m 11.0s --> 0.355s for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base = classification_data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort them to avoid taking the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_sorted = sorted(meters, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in classification_data_df['Base Prediction'].values:\n",
    "#     for meter in meters_sorted: \n",
    "#         if meter in res:\n",
    "            \n",
    "\n",
    "# cls_df_base.loc[cls_df_base['Base Prediction'] in , 'Base Prediction'] = cls_df_base['Base Res'] * 2\n",
    "changed = []\n",
    "for idx, res in enumerate(classification_data_df['Base Prediction'].values):\n",
    "    for meter in meters_sorted:\n",
    "        if meter in res:\n",
    "            ch_dict = {'Index': idx, \"Before\": res, 'After': meter}\n",
    "            changed.append(ch_dict)\n",
    "            classification_data_df.at[idx, 'Base Prediction'] = meter\n",
    "print(changed)\n",
    "print(len(changed))\n",
    "print(len(classification_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Base Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy without Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Base Prediction'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_pattern = '[\\u0621-\\u064A]'\n",
    "\n",
    "# Filter rows where 'Base Prediction' does not contain Arabic letters\n",
    "cls_df_without_arabic = classification_data_df[classification_data_df['Base Prediction'].apply(lambda x: not bool(re.search(arabic_pattern, x)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = cls_df_without_empty['output'] == cls_df_without_empty['Base Prediction']\n",
    "accuracy_without_empty = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model (Input refined for API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df = pd.read_json(\"test_cls_final.json\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'] = 'Pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classification_data_df)):\n",
    "    classification_data_df.iloc[i,2] = get_response(classification_data_df.iloc[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying 200 samples took 1m 37.3s --> 0.355s for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base = classification_data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort them to avoid taking the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_sorted = sorted(meters, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in classification_data_df['Base Prediction'].values:\n",
    "#     for meter in meters_sorted: \n",
    "#         if meter in res:\n",
    "            \n",
    "\n",
    "# cls_df_base.loc[cls_df_base['Base Prediction'] in , 'Base Prediction'] = cls_df_base['Base Res'] * 2\n",
    "changed = []\n",
    "for idx, res in enumerate(classification_data_df['Base Prediction'].values):\n",
    "    for meter in meters_sorted:\n",
    "        if meter in res:\n",
    "            ch_dict = {'Index': idx, \"Before\": res, 'After': meter}\n",
    "            changed.append(ch_dict)\n",
    "            classification_data_df.at[idx, 'Base Prediction'] = meter\n",
    "print(changed)\n",
    "print(len(changed))\n",
    "print(len(classification_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df['Base Prediction'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Base Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy without Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Base Prediction'] != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_pattern = '[\\u0621-\\u064A]'\n",
    "\n",
    "# Filter rows where 'Base Prediction' does not contain Arabic letters\n",
    "cls_df_without_arabic = classification_data_df[classification_data_df['Base Prediction'].apply(lambda x: not bool(re.search(arabic_pattern, x)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = cls_df_without_empty['output'] == cls_df_without_empty['Base Prediction']\n",
    "accuracy_without_empty = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_cls_formatted.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()  \n",
    "    \n",
    "fine_tuned_preds =  [line.strip() for line in lines]\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [\n",
    "    ' '.join(line.replace('\\n', '').split()) for line in fine_tuned_preds\n",
    "]\n",
    "\n",
    "print(fine_tuned_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [re.sub(r'[^\\u0600-\\u06FF]+', '', pred).strip() for pred in fine_tuned_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in fine_tuned_preds:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[\"Fine-Tuning Prediction\"] = fine_tuned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[classification_data_df['Fine-Tuning Prediction'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Fine-Tuning Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "wrong_rows_df = classification_data_df[~matches]\n",
    "print(\"Rows where col1 and col2 don't match:\")\n",
    "print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [\n",
    "    ' '.join(line.replace('\\n', '').split()) for line in fine_tuned_preds\n",
    "]\n",
    "\n",
    "print(fine_tuned_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [re.sub(r'[^\\u0600-\\u06FF]+', '', pred).strip() for pred in fine_tuned_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in fine_tuned_preds:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[\"Fine-Tuning Prediction\"] = fine_tuned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[classification_data_df['Fine-Tuning Prediction'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Fine-Tuning Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "wrong_rows_df = classification_data_df[~matches]\n",
    "print(\"Rows where col1 and col2 don't match:\")\n",
    "print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4o Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GPT_4o_Cls.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()  \n",
    "    \n",
    "gpt_preds =  [line.strip() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_3628\\2414898903.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  classification_data_df_100[\"GPT_4o Prediction\"] = gpt_preds\n"
     ]
    }
   ],
   "source": [
    "classification_data_df_100 = classification_data_df.head(100)\n",
    "\n",
    "classification_data_df_100[\"GPT_4o Prediction\"] = gpt_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.00%\n"
     ]
    }
   ],
   "source": [
    "matches = classification_data_df_100['output'] == classification_data_df_100['GPT_4o Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [\n",
    "    ' '.join(line.replace('\\n', '').split()) for line in fine_tuned_preds\n",
    "]\n",
    "\n",
    "print(fine_tuned_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_preds = [re.sub(r'[^\\u0600-\\u06FF]+', '', pred).strip() for pred in fine_tuned_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in fine_tuned_preds:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[\"Fine-Tuning Prediction\"] = fine_tuned_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_df[classification_data_df['Fine-Tuning Prediction'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = classification_data_df['output'] == classification_data_df['Fine-Tuning Prediction']\n",
    "accuracy = matches.mean() * 100  \n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "wrong_rows_df = classification_data_df[~matches]\n",
    "print(\"Rows where col1 and col2 don't match:\")\n",
    "print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_without_empty = classification_data_df[classification_data_df['Fine-Tuning Prediction'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_without_empty = cls_df_without_empty['output'] == cls_df_without_empty['Fine-Tuning Prediction']\n",
    "accuracy_without_empty  = matches_without_empty .mean() * 100  \n",
    "print(f\"Accuracy: {accuracy_without_empty:.2f}%\")\n",
    "\n",
    "# wrong_rows_df = cls_df_without_empty[~matches]\n",
    "# print(\"Rows where col1 and col2 don't match:\")\n",
    "# print(wrong_rows_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_df_base['output'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
